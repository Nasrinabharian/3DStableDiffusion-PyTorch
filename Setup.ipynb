{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b2e0170-511f-419f-a5d0-003fe1b5bcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'StableDiffusion-PyTorch'...\n",
      "remote: Enumerating objects: 119, done.\u001b[K\n",
      "remote: Counting objects: 100% (119/119), done.\u001b[K\n",
      "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
      "remote: Total 119 (delta 60), reused 67 (delta 24), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (119/119), 49.63 KiB | 1.34 MiB/s, done.\n",
      "Resolving deltas: 100% (60/60), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/explainingai-code/StableDiffusion-PyTorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e090627a-6539-4f1a-8ff7-ce808c05a0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/abharian/LDM_Project/StableDiffusion-PyTorch\n"
     ]
    }
   ],
   "source": [
    "cd /Users/abharian/LDM_Project/StableDiffusion-PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4edcad7-3db1-47e5-acd7-d495dc24eb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3==1.34.36 (from -r requirements.txt (line 1))\n",
      "  Using cached boto3-1.34.36-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting botocore==1.34.36 (from -r requirements.txt (line 2))\n",
      "  Using cached botocore-1.34.36-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting certifi==2023.7.22 (from -r requirements.txt (line 3))\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting charset-normalizer==3.2.0 (from -r requirements.txt (line 4))\n",
      "  Using cached charset_normalizer-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: click==8.1.7 in /Users/abharian/.conda/envs/vname/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (8.1.7)\n",
      "Collecting contourpy==1.1.0 (from -r requirements.txt (line 6))\n",
      "  Using cached contourpy-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting cycler==0.11.0 (from -r requirements.txt (line 7))\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl.metadata (785 bytes)\n",
      "Collecting einops==0.6.1 (from -r requirements.txt (line 8))\n",
      "  Using cached einops-0.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock==3.13.1 in /Users/abharian/.conda/envs/vname/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (3.13.1)\n",
      "Collecting fonttools==4.42.0 (from -r requirements.txt (line 10))\n",
      "  Using cached fonttools-4.42.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (150 kB)\n",
      "Collecting fsspec==2024.2.0 (from -r requirements.txt (line 11))\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting ftfy==6.1.3 (from -r requirements.txt (line 12))\n",
      "  Using cached ftfy-6.1.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting huggingface-hub==0.20.3 (from -r requirements.txt (line 13))\n",
      "  Using cached huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: idna==3.4 in /Users/abharian/.conda/envs/vname/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (3.4)\n",
      "Requirement already satisfied: importlib-metadata==7.0.1 in /Users/abharian/.conda/envs/vname/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (7.0.1)\n",
      "Collecting importlib-resources==6.0.1 (from -r requirements.txt (line 16))\n",
      "  Using cached importlib_resources-6.0.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting jmespath==1.0.1 (from -r requirements.txt (line 17))\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting joblib==1.3.2 (from -r requirements.txt (line 18))\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting kiwisolver==1.4.4 (from -r requirements.txt (line 19))\n",
      "  Using cached kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting matplotlib==3.7.2 (from -r requirements.txt (line 20))\n",
      "  Using cached matplotlib-3.7.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting numpy==1.23.5 (from -r requirements.txt (line 21))\n",
      "  Using cached numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting opencv-python==4.8.0.74 (from -r requirements.txt (line 22))\n",
      "  Using cached opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting packaging==23.1 (from -r requirements.txt (line 23))\n",
      "  Using cached packaging-23.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting Pillow==10.0.0 (from -r requirements.txt (line 24))\n",
      "  Using cached Pillow-10.0.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting pyparsing==3.0.9 (from -r requirements.txt (line 25))\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /Users/abharian/.conda/envs/vname/lib/python3.8/site-packages (from -r requirements.txt (line 26)) (2.8.2)\n",
      "Collecting PyYAML==6.0 (from -r requirements.txt (line 27))\n",
      "  Using cached PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting regex==2023.12.25 (from -r requirements.txt (line 28))\n",
      "  Using cached regex-2023.12.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests==2.31.0 in /Users/abharian/.conda/envs/vname/lib/python3.8/site-packages (from -r requirements.txt (line 29)) (2.31.0)\n",
      "Collecting s3transfer==0.10.0 (from -r requirements.txt (line 30))\n",
      "  Using cached s3transfer-0.10.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting sacremoses==0.1.1 (from -r requirements.txt (line 31))\n",
      "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting safetensors==0.4.2 (from -r requirements.txt (line 32))\n",
      "  Using cached safetensors-0.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: scipy==1.10.1 in /Users/abharian/.conda/envs/vname/lib/python3.8/site-packages (from -r requirements.txt (line 33)) (1.10.1)\n",
      "Collecting sentencepiece==0.1.99 (from -r requirements.txt (line 34))\n",
      "  Using cached sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: six==1.16.0 in /Users/abharian/.conda/envs/vname/lib/python3.8/site-packages (from -r requirements.txt (line 35)) (1.16.0)\n",
      "Collecting tokenizers==0.15.1 (from -r requirements.txt (line 36))\n",
      "  Using cached tokenizers-0.15.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting torch==1.11.0 (from -r requirements.txt (line 37))\n",
      "  Using cached torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting torchvision==0.12.0 (from -r requirements.txt (line 38))\n",
      "  Using cached torchvision-0.12.0-cp38-cp38-manylinux1_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tqdm==4.65.0 in /Users/abharian/.conda/envs/vname/lib/python3.8/site-packages (from -r requirements.txt (line 39)) (4.65.0)\n",
      "Collecting transformers==4.37.2 (from -r requirements.txt (line 40))\n",
      "  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "Collecting typing_extensions==4.7.1 (from -r requirements.txt (line 41))\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting urllib3==1.26.18 (from -r requirements.txt (line 42))\n",
      "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "Collecting wcwidth==0.2.13 (from -r requirements.txt (line 43))\n",
      "  Using cached wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zipp==3.16.2 (from -r requirements.txt (line 44))\n",
      "  Using cached zipp-3.16.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Using cached boto3-1.34.36-py3-none-any.whl (139 kB)\n",
      "Using cached botocore-1.34.36-py3-none-any.whl (11.9 MB)\n",
      "Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Using cached charset_normalizer-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "Using cached contourpy-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Using cached einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "Using cached fonttools-4.42.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Using cached ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "Using cached huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "Using cached importlib_resources-6.0.1-py3-none-any.whl (34 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "Using cached matplotlib-3.7.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "Using cached numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "Using cached opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
      "Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Using cached Pillow-10.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Using cached PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "Using cached regex-2023.12.25-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777 kB)\n",
      "Using cached s3transfer-0.10.0-py3-none-any.whl (82 kB)\n",
      "Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "Using cached safetensors-0.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tokenizers-0.15.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached torch-1.11.0-cp38-cp38-manylinux1_x86_64.whl (750.6 MB)\n",
      "Using cached torchvision-0.12.0-cp38-cp38-manylinux1_x86_64.whl (21.0 MB)\n",
      "Using cached transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "Using cached wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Using cached zipp-3.16.2-py3-none-any.whl (7.2 kB)\n",
      "Installing collected packages: wcwidth, sentencepiece, zipp, urllib3, typing_extensions, safetensors, regex, PyYAML, pyparsing, Pillow, packaging, numpy, kiwisolver, joblib, jmespath, ftfy, fsspec, fonttools, einops, cycler, charset-normalizer, certifi, torch, sacremoses, opencv-python, importlib-resources, contourpy, botocore, torchvision, s3transfer, matplotlib, huggingface-hub, tokenizers, boto3, transformers\n",
      "  Attempting uninstall: wcwidth\n",
      "    Found existing installation: wcwidth 0.2.5\n",
      "    Uninstalling wcwidth-0.2.5:\n",
      "      Successfully uninstalled wcwidth-0.2.5\n",
      "  Attempting uninstall: zipp\n",
      "    Found existing installation: zipp 3.17.0\n",
      "    Uninstalling zipp-3.17.0:\n",
      "      Successfully uninstalled zipp-3.17.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.1.0\n",
      "    Uninstalling urllib3-2.1.0:\n",
      "      Successfully uninstalled urllib3-2.1.0\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: pyparsing\n",
      "    Found existing installation: pyparsing 3.1.2\n",
      "    Uninstalling pyparsing-3.1.2:\n",
      "      Successfully uninstalled pyparsing-3.1.2\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 10.2.0\n",
      "    Uninstalling pillow-10.2.0:\n",
      "      Successfully uninstalled pillow-10.2.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.2\n",
      "    Uninstalling packaging-23.2:\n",
      "      Successfully uninstalled packaging-23.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: kiwisolver\n",
      "    Found existing installation: kiwisolver 1.4.5\n",
      "    Uninstalling kiwisolver-1.4.5:\n",
      "      Successfully uninstalled kiwisolver-1.4.5\n",
      "  Attempting uninstall: fonttools\n",
      "    Found existing installation: fonttools 4.50.0\n",
      "    Uninstalling fonttools-4.50.0:\n",
      "      Successfully uninstalled fonttools-4.50.0\n",
      "  Attempting uninstall: cycler\n",
      "    Found existing installation: cycler 0.12.1\n",
      "    Uninstalling cycler-0.12.1:\n",
      "      Successfully uninstalled cycler-0.12.1\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.0.4\n",
      "    Uninstalling charset-normalizer-2.0.4:\n",
      "      Successfully uninstalled charset-normalizer-2.0.4\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.2.2\n",
      "    Uninstalling certifi-2024.2.2:\n",
      "      Successfully uninstalled certifi-2024.2.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.1\n",
      "    Uninstalling torch-2.2.1:\n",
      "      Successfully uninstalled torch-2.2.1\n",
      "  Attempting uninstall: importlib-resources\n",
      "    Found existing installation: importlib-resources 6.1.1\n",
      "    Uninstalling importlib-resources-6.1.1:\n",
      "      Successfully uninstalled importlib-resources-6.1.1\n",
      "  Attempting uninstall: contourpy\n",
      "    Found existing installation: contourpy 1.1.1\n",
      "    Uninstalling contourpy-1.1.1:\n",
      "      Successfully uninstalled contourpy-1.1.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.17.1\n",
      "    Uninstalling torchvision-0.17.1:\n",
      "      Successfully uninstalled torchvision-0.17.1\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.5\n",
      "    Uninstalling matplotlib-3.7.5:\n",
      "      Successfully uninstalled matplotlib-3.7.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.13.1 requires flatbuffers>=23.1.21, but you have flatbuffers 2.0 which is incompatible.\n",
      "tensorflow 2.13.1 requires tensorboard<2.14,>=2.13, but you have tensorboard 2.12.1 which is incompatible.\n",
      "tensorflow 2.13.1 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Pillow-10.0.0 PyYAML-6.0 boto3-1.34.36 botocore-1.34.36 certifi-2023.7.22 charset-normalizer-3.2.0 contourpy-1.1.0 cycler-0.11.0 einops-0.6.1 fonttools-4.42.0 fsspec-2024.2.0 ftfy-6.1.3 huggingface-hub-0.20.3 importlib-resources-6.0.1 jmespath-1.0.1 joblib-1.3.2 kiwisolver-1.4.4 matplotlib-3.7.2 numpy-1.23.5 opencv-python-4.8.0.74 packaging-23.1 pyparsing-3.0.9 regex-2023.12.25 s3transfer-0.10.0 sacremoses-0.1.1 safetensors-0.4.2 sentencepiece-0.1.99 tokenizers-0.15.1 torch-1.11.0 torchvision-0.12.0 transformers-4.37.2 typing_extensions-4.7.1 urllib3-1.26.18 wcwidth-0.2.13 zipp-3.16.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b2d2a80-593d-4049-b702-bdddc44d64e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Using cached wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=966153f810c562cc4f3645506a6bb72878698fcb0a4766c80cef70b03ba17949\n",
      "  Stored in directory: /Users/abharian/.cache/pip/wheels/bd/a8/c3/3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c5685e0-0c73-4e3d-99d6-c76e49f2e82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading LPIPS weights...\n",
      "\n",
      "LPIPS weights downloaded successfully!\n",
      "LPIPS weights saved to: /Users/abharian/LDM_Project/StableDiffusion-PyTorch/models/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wget\n",
    "\n",
    "# Define the URLs\n",
    "lpips_weights_url = 'https://github.com/richzhang/PerceptualSimilarity/raw/master/lpips/weights/v0.1/vgg.pth'\n",
    "destination_directory = '/Users/abharian/LDM_Project/StableDiffusion-PyTorch/models/weights/v0.1/'\n",
    "destination_file = os.path.join(destination_directory, 'vgg.pth')\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "#os.makedirs(destination_directory, exist_ok=True)\n",
    "\n",
    "# Download the weights file\n",
    "print(\"Downloading LPIPS weights...\")\n",
    "wget.download(lpips_weights_url, out=destination_file)\n",
    "print(\"\\nLPIPS weights downloaded successfully!\")\n",
    "\n",
    "# Check if the file exists in the destination directory\n",
    "if os.path.exists(destination_file):\n",
    "    print(f\"LPIPS weights saved to: {destination_file}\")\n",
    "else:\n",
    "    print(\"Error: Failed to download LPIPS weights.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b9fbcb2-3852-4d58-acaf-d0536c344fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_params': {'im_path': '/Users/abharian/LDM_Project/StableDiffusion-PyTorch/data/mnist/train/images', 'im_channels': 1, 'im_size': [511, 351, 119], 'name': 'mnist'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0015, 'beta_end': 0.0195}, 'ldm_params': {'down_channels': [128, 256, 256, 256], 'mid_channels': [256, 256], 'down_sample': [False, False, False], 'attn_down': [True, True, True], 'time_emb_dim': 256, 'norm_channels': 32, 'num_heads': 16, 'conv_out_channels': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2, 'condition_config': {'condition_types': ['class'], 'class_condition_config': {'num_classes': 3, 'cond_drop_prob': 0.1}}}, 'autoencoder_params': {'z_channels': 3, 'codebook_size': 20, 'down_channels': [32, 64, 128], 'mid_channels': [128, 128], 'down_sample': [True, True], 'attn_down': [False, False], 'norm_channels': [128, 128, 128], 'num_heads': 16, 'num_down_layers': 1, 'num_mid_layers': 1, 'num_up_layers': 1}, 'train_params': {'seed': 1111, 'task_name': 'mnist', 'ldm_batch_size': 1, 'autoencoder_batch_size': 1, 'disc_start': 1000, 'disc_weight': 0.5, 'codebook_weight': 1, 'commitment_beta': 0.2, 'perceptual_weight': 1, 'kl_weight': 5e-06, 'ldm_epochs': 100, 'autoencoder_epochs': 50, 'num_samples': 25, 'num_grid_rows': 1, 'ldm_lr': 1e-05, 'autoencoder_lr': 0.0001, 'autoencoder_acc_steps': 1, 'autoencoder_img_save_steps': 8, 'save_latents': False, 'cf_guidance_scale': 1.0, 'vae_latent_dir_name': 'vae_latents', 'vqvae_latent_dir_name': 'vqvae_latents', 'ldm_ckpt_name': 'ddpm_ckpt_class_cond.pth', 'vqvae_autoencoder_ckpt_name': 'vqvae_autoencoder_ckpt.pth', 'vae_autoencoder_ckpt_name': 'vae_autoencoder_ckpt.pth', 'vqvae_discriminator_ckpt_name': 'vqvae_discriminator_ckpt.pth', 'vae_discriminator_ckpt_name': 'vae_discriminator_ckpt.pth'}}\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 4498.72it/s]\n",
      "Found 94 images for split train\n",
      "Loading model from: /Users/abharian/LDM_Project/StableDiffusion-PyTorch/models/weights/v0.1/vgg.pth\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abharian/.conda/envs/vname/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/abharian/.conda/envs/vname/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/abharian/LDM_Project/StableDiffusion-PyTorch/tools/train_vqvae.py\", line 208, in <module>\n",
      "    train(args)\n",
      "  File \"/Users/abharian/LDM_Project/StableDiffusion-PyTorch/tools/train_vqvae.py\", line 75, in train\n",
      "    lpips_model = LPIPS().eval().to(device)\n",
      "  File \"/Users/abharian/LDM_Project/StableDiffusion-PyTorch/models/lpips.py\", line 94, in __init__\n",
      "    self.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n",
      "  File \"/Users/abharian/.conda/envs/vname/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1497, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for LPIPS:\n",
      "\tsize mismatch for lin0.model.1.weight: copying a param with shape torch.Size([1, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 64, 1, 1, 1]).\n",
      "\tsize mismatch for lin1.model.1.weight: copying a param with shape torch.Size([1, 128, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 128, 1, 1, 1]).\n",
      "\tsize mismatch for lin2.model.1.weight: copying a param with shape torch.Size([1, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 256, 1, 1, 1]).\n",
      "\tsize mismatch for lin3.model.1.weight: copying a param with shape torch.Size([1, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 512, 1, 1, 1]).\n",
      "\tsize mismatch for lin4.model.1.weight: copying a param with shape torch.Size([1, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 512, 1, 1, 1]).\n"
     ]
    }
   ],
   "source": [
    "!python -m tools.train_vqvae --config /Users/abharian/LDM_Project/StableDiffusion-PyTorch/config/mnist_class_cond.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183a5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#till"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d4fcef-10f0-4f2a-b792-0337a5ace625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_params': {'im_path': '/Users/abharian/LDM_Project/StableDiffusion-PyTorch/data/mnist/train/images', 'im_channels': 1, 'im_size': [511, 351, 119], 'name': 'mnist'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0015, 'beta_end': 0.0195}, 'ldm_params': {'down_channels': [128, 256, 256, 256], 'mid_channels': [256, 256], 'down_sample': [False, False, False], 'attn_down': [True, True, True], 'time_emb_dim': 256, 'norm_channels': 32, 'num_heads': 16, 'conv_out_channels': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2, 'condition_config': {'condition_types': ['class'], 'class_condition_config': {'num_classes': 3, 'cond_drop_prob': 0.1}}}, 'autoencoder_params': {'z_channels': 3, 'codebook_size': 20, 'down_channels': [32, 64, 128], 'mid_channels': [128, 128], 'down_sample': [True, True], 'attn_down': [False, False], 'norm_channels': [128, 128, 128], 'num_heads': 16, 'num_down_layers': 1, 'num_mid_layers': 1, 'num_up_layers': 1}, 'train_params': {'seed': 1111, 'task_name': 'mnist', 'ldm_batch_size': 1, 'autoencoder_batch_size': 1, 'disc_start': 1000, 'disc_weight': 0.5, 'codebook_weight': 1, 'commitment_beta': 0.2, 'perceptual_weight': 1, 'kl_weight': 5e-06, 'ldm_epochs': 100, 'autoencoder_epochs': 50, 'num_samples': 25, 'num_grid_rows': 1, 'ldm_lr': 1e-05, 'autoencoder_lr': 0.0001, 'autoencoder_acc_steps': 1, 'autoencoder_img_save_steps': 8, 'save_latents': False, 'cf_guidance_scale': 1.0, 'vae_latent_dir_name': 'vae_latents', 'vqvae_latent_dir_name': 'vqvae_latents', 'ldm_ckpt_name': 'ddpm_ckpt_class_cond.pth', 'vqvae_autoencoder_ckpt_name': 'vqvae_autoencoder_ckpt.pth', 'vae_autoencoder_ckpt_name': 'vae_autoencoder_ckpt.pth', 'vqvae_discriminator_ckpt_name': 'vqvae_discriminator_ckpt.pth', 'vae_discriminator_ckpt_name': 'vae_discriminator_ckpt.pth'}}\n",
      "100%|███████████████████████████████████████████| 3/3 [00:00<00:00, 5565.20it/s]\n",
      "Found 94 images for split train\n",
      "Latents not found\n",
      "Loading vqvae model as latents not present\n",
      "Loaded vae checkpoint\n",
      "  0%|                                                    | 0/94 [00:00<?, ?it/s]Keys in the .npz file: ['arr_0']\n",
      "Extracted array 'Segmented_intensity' from 'arr_0' with shape (511, 351, 119)\n",
      "  0%|                                                    | 0/94 [00:01<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/abharian/.conda/envs/vname/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/abharian/.conda/envs/vname/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/abharian/LDM_Project/StableDiffusion-PyTorch/tools/train_ddpm_cond.py\", line 189, in <module>\n",
      "    train(args)\n",
      "  File \"/Users/abharian/LDM_Project/StableDiffusion-PyTorch/tools/train_ddpm_cond.py\", line 127, in train\n",
      "    im, _ = vae.encode(im)\n",
      "  File \"/Users/abharian/LDM_Project/StableDiffusion-PyTorch/models/vqvae.py\", line 109, in encode\n",
      "    out = down(out)\n",
      "  File \"/Users/abharian/.conda/envs/vname/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/Users/abharian/LDM_Project/StableDiffusion-PyTorch/models/blocks.py\", line 116, in forward\n",
      "    out = self.resnet_conv_first[i](out)\n",
      "  File \"/Users/abharian/.conda/envs/vname/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/Users/abharian/.conda/envs/vname/lib/python3.8/site-packages/torch/nn/modules/container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"/Users/abharian/.conda/envs/vname/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1110, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/Users/abharian/.conda/envs/vname/lib/python3.8/site-packages/torch/nn/modules/normalization.py\", line 268, in forward\n",
      "    return F.group_norm(\n",
      "  File \"/Users/abharian/.conda/envs/vname/lib/python3.8/site-packages/torch/nn/functional.py\", line 2498, in group_norm\n",
      "    _verify_batch_size([input.size(0) * input.size(1) // num_groups, num_groups] + list(input.size()[2:]))\n",
      "TypeError: unsupported operand type(s) for //: 'int' and 'list'\n"
     ]
    }
   ],
   "source": [
    "!python -m tools.train_ddpm_cond --config /Users/abharian/LDM_Project/StableDiffusion-PyTorch/config/mnist_class_cond.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70552320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#till2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07d356bb-fc7c-4a83-ac42-049096b47bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_params': {'im_path': '/Users/abharian/LDM_Project/StableDiffusion-PyTorch/data/mnist/train/images', 'im_channels': 3, 'im_size': 128, 'name': 'mnist'}, 'diffusion_params': {'num_timesteps': 1000, 'beta_start': 0.0015, 'beta_end': 0.0195}, 'ldm_params': {'down_channels': [128, 256, 256, 256], 'mid_channels': [256, 256], 'down_sample': [False, False, False], 'attn_down': [True, True, True], 'time_emb_dim': 256, 'norm_channels': 32, 'num_heads': 16, 'conv_out_channels': 128, 'num_down_layers': 2, 'num_mid_layers': 2, 'num_up_layers': 2, 'condition_config': {'condition_types': ['class'], 'class_condition_config': {'num_classes': 3, 'cond_drop_prob': 0.1}}}, 'autoencoder_params': {'z_channels': 3, 'codebook_size': 20, 'down_channels': [32, 64, 128], 'mid_channels': [128, 128], 'down_sample': [True, True], 'attn_down': [False, False], 'norm_channels': 32, 'num_heads': 16, 'num_down_layers': 1, 'num_mid_layers': 1, 'num_up_layers': 1}, 'train_params': {'seed': 1111, 'task_name': 'mnist', 'ldm_batch_size': 1, 'autoencoder_batch_size': 1, 'disc_start': 1000, 'disc_weight': 0.5, 'codebook_weight': 1, 'commitment_beta': 0.2, 'perceptual_weight': 1, 'kl_weight': 5e-06, 'ldm_epochs': 100, 'autoencoder_epochs': 50, 'num_samples': 25, 'num_grid_rows': 1, 'ldm_lr': 1e-05, 'autoencoder_lr': 0.0001, 'autoencoder_acc_steps': 1, 'autoencoder_img_save_steps': 8, 'save_latents': False, 'cf_guidance_scale': 1.0, 'vae_latent_dir_name': 'vae_latents', 'vqvae_latent_dir_name': 'vqvae_latents', 'ldm_ckpt_name': 'ddpm_ckpt_class_cond.pth', 'vqvae_autoencoder_ckpt_name': 'vqvae_autoencoder_ckpt.pth', 'vae_autoencoder_ckpt_name': 'vae_autoencoder_ckpt.pth', 'vqvae_discriminator_ckpt_name': 'vqvae_discriminator_ckpt.pth', 'vae_discriminator_ckpt_name': 'vae_discriminator_ckpt.pth'}}\n",
      "Loaded unet checkpoint\n",
      "Loaded vae checkpoint\n",
      "Generating images for [2, 2, 0, 0, 0, 0, 2, 1, 2, 1, 0, 0, 2, 2, 0, 1, 1, 2, 1, 2, 0, 1, 0, 2, 0]\n",
      "1000it [05:31,  3.02it/s]\n"
     ]
    }
   ],
   "source": [
    "!python -m tools.sample_ddpm_class_cond --config /Users/abharian/LDM_Project/StableDiffusion-PyTorch/config/mnist_class_cond.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e751ba13-bf5f-4b2b-8f6f-9425ac0a1294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e26496b-d487-4a70-a008-faa64e01e984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing complete. Resized images are saved in the 'resized_images' folder.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os \n",
    "\n",
    "# Folder paths\n",
    "input_folder = \"/Users/abharian/train/2/\"\n",
    "output_folder = \"/Users/abharian/LDM_Project/StableDiffusion-PyTorch/data/mnist/train/images/2/\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Desired dimensions for resized images (width, height)\n",
    "new_size = (28, 28)  # Adjust these values to your desired dimensions\n",
    "\n",
    "# Iterate through all image files in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith((\".jpg\", \".jpeg\", \".png\", \".gif\")):\n",
    "        # Open the image\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        image = Image.open(input_path).convert(\"RGB\")  # Open as RGB\n",
    "        \n",
    "        # Resize the image\n",
    "        resized_image = image.resize(new_size)\n",
    "\n",
    "        # Save the resized image in the output folder\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        resized_image.save(output_path)\n",
    "\n",
    "print(\"Resizing complete. Resized images are saved in the 'resized_images' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e3aa848-cea2-47a7-b548-694161a907c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.19\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280c8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(tensorflow3)",
   "language": "python",
   "name": "tensorflow3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
